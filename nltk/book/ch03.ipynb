{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6   Normalizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import *\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords \n",
    "is no basis for a system of government.  Supreme executive power derives from \n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'DENNI', u':', u'Listen', u',', u'strang', u'women', u'lie', u'in', u'pond', u'distribut', u'sword', u'is', u'no', u'basi', u'for', u'a', u'system', u'of', u'govern', u'.', u'Suprem', u'execut', u'power', u'deriv', u'from', u'a', u'mandat', u'from', u'the', u'mass', u',', u'not', u'from', u'some', u'farcic', u'aquat', u'ceremoni', u'.']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "print [porter.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "print [lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WordNet lemmatizer only removes affixes if the resulting word is in its dictionary. This additional checking process makes the lemmatizer slower than the above stemmers. Notice that it doesn't handle lying, but it converts women to woman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', u'woman', 'lying', 'in', u'pond', 'distributing', u'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', u'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "print [wnl.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7   Regular Expressions for Tokenizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function nltk.regexp_tokenize() is similar to re.findall() (as we've been using it for tokenization). However, nltk.regexp_tokenize() is more efficient for this task, and avoids the need for special treatment of parentheses. For readability we break up the regular expression over several lines and add a comment about each line. The special (?x) \"verbose flag\" tells Python to strip out the embedded whitespace and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('', '', ''), ('A.', '', ''), ('', '-print', ''), ('', '', ''), ('', '', '.40'), ('', '', '')]\n"
     ]
    }
   ],
   "source": [
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "pattern = r'''(?x)    # set flag to allow verbose regexps\n",
    "    ([A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "  | \\w+(-\\w+)*        # words with optional internal hyphens\n",
    "  | \\$?\\d+(\\.\\d+)?%?  # currency and percentages, e.g. $12.40, 82%\n",
    "  | \\.\\.\\.            # ellipsis\n",
    "  | [][.,;\"'?():-_`]  # these are separate tokens; includes ], [\n",
    "'''\n",
    "\n",
    "print regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8   Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'\"Nonsense!\"',\n",
       " u'said Gregory, who was very rational when anyone else\\nattempted paradox.',\n",
       " u'\"Why do all the clerks and navvies in the\\nrailway trains look so sad and tired, so very sad and tired?',\n",
       " u'I will\\ntell you.',\n",
       " u'It is because they know that the train is going right.',\n",
       " u'It\\nis because they know that whatever place they have taken a ticket\\nfor that place they will reach.',\n",
       " u'It is because after they have\\npassed Sloane Square they know that the next station must be\\nVictoria, and nothing but Victoria.',\n",
       " u'Oh, their wild rapture!',\n",
       " u'oh,\\ntheir eyes like stars and their souls again in Eden, if the next\\nstation were unaccountably Baker Street!\"',\n",
       " u'\"It is you who are unpoetical,\" replied the poet Syme.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\n",
    "sents = nltk.sent_tokenize(text)\n",
    "sents[79:89]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10   Summary\n",
    "\n",
    "* In this book we view a text as a list of words. A \"raw text\" is a potentially long string containing words and whitespace formatting, and is how we typically store and visualize a text.\n",
    "* A string is specified in Python using single or double quotes: 'Monty Python', \"Monty Python\".\n",
    "* The characters of a string are accessed using indexes, counting from zero: 'Monty Python'[0] gives the value M. The length of a string is found using len().\n",
    "* Substrings are accessed using slice notation: 'Monty Python'[1:5] gives the value onty. If the start index is omitted, the substring begins at the start of the string; if the end index is omitted, the slice continues to the end of the string.\n",
    "* Strings can be split into lists: 'Monty Python'.split() gives ['Monty', 'Python']. Lists can be joined into strings: '/'.join(['Monty', 'Python']) gives 'Monty/Python'.\n",
    "* We can read text from a file input.txt using text = open('input.txt').read(). We can read text from url using text = request.urlopen(url).read().decode('utf8'). We can iterate over the lines of a text file using for line in open(f).\n",
    "* We can write text to a file by opening the file for writing output_file = open('output.txt', 'w'), then adding content to the file print(\"Monty Python\", file=output_file).\n",
    "* Texts found on the web may contain unwanted material (such as headers, footers, markup), that need to be removed before we do any linguistic processing.\n",
    "* Tokenization is the segmentation of a text into basic units — or tokens — such as words and punctuation. Tokenization based on whitespace is inadequate for many applications because it bundles punctuation together with words. NLTK provides an off-the-shelf tokenizer nltk.word_tokenize().\n",
    "* Lemmatization is a process that maps the various forms of a word (such as appeared, appears) to the canonical or citation form of the word, also known as the lexeme or lemma (e.g. appear).\n",
    "* Regular expressions are a powerful and flexible method of specifying patterns. Once we have imported the re module, we can use re.findall() to find all substrings in a string that match a pattern.\n",
    "* If a regular expression string includes a backslash, you should tell Python not to preprocess the string, by using a raw string with an r prefix: r'regexp'.\n",
    "* When backslash is used before certain characters, e.g. \\n, this takes on a special meaning (newline character); however, when backslash is used before regular expression wildcards and operators, e.g. \\., \\|, \\$, these characters lose their special meaning and are matched literally.\n",
    "* A string formatting expression template % arg_tuple consists of a format string template that contains conversion specifiers like %-6s and %0.2d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
