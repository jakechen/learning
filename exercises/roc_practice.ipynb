{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC and PR for Unbalanced Datasets\n",
    "\n",
    "Practice problems from Bill@Deloitte.\n",
    "\n",
    "Tasks:\n",
    "0. Set up an unbalanced dataset\n",
    "1. Plot an ROC Curve\n",
    "2. Plot precision as a function of TPR (aka PR curve)\n",
    "3. Given priors, find operating point.\n",
    "4. Confirm that PR depends on tested priors, but ROC does not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up an unbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method to create the unbalanced dataset\n",
    "def make_unbalanced_blobs(pos_prob):\n",
    "    from sklearn.datasets import make_blobs\n",
    "\n",
    "    X, y = make_blobs(n_samples=20000, cluster_std=5, centers=2, random_state=123)\n",
    "\n",
    "    temp = []\n",
    "    for i in range(len(X)):\n",
    "        if y[i]==0 and np.random.uniform()>pos_prob:\n",
    "            temp.append([X[i][0], X[i][1], y[i]])\n",
    "        elif y[i]==1 and np.random.uniform()<=pos_prob:\n",
    "            temp.append([X[i][0], X[i][1], y[i]])\n",
    "\n",
    "    X = np.array([[t[0], t[1]] for t in temp])\n",
    "    y = np.array([t[2] for t in temp])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate an unbalanced dataset with 10% positive\n",
    "pos_prob = 0.10\n",
    "X, y = make_unbalanced_blobs(pos_prob)\n",
    "\n",
    "# view a sample dataset\n",
    "x1 = [i[0] for i in X]\n",
    "x2 = [i[1] for i in X]\n",
    "\n",
    "plt.scatter(x1, x2, c=y)\n",
    "plt.title(\"x2 vs x1 in unbalanced dataset\")\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a little bit of overlap in our dummy data, which will give the classifier a little trouble. This is good, or else we will have a perfect classifier and thus a very boring ROC curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now split our dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split train and test datasets\n",
    "train_proportion = 0.75\n",
    "train_size = int(len(X)*train_proportion)\n",
    "X_train = X[train_size:]\n",
    "y_train = y[train_size:]\n",
    "X_test = X[:train_size]\n",
    "y_test = y[:train_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data split we now train the model on the training subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import classifier and build model on first 7500 samples\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have build the model on the training set, let's see what at what probability the model predicts for each of the test set records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict class for last 2500 samples\n",
    "y_pred_proba = classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the probabilities for the first 5 test set records. In each element the first number is the probability that y==0 and the second is probability that y==1. Notice each element adds to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print y_pred_proba[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move on to the exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Plot an ROC Curve\n",
    "\n",
    "An ROC curve is the plot of the FPR vs TPR of a classifier as the discrimation threshold is increased for 0% to 100%.\n",
    "\n",
    "To plot an ROC curve we first have to obtain results from a classification problem. For this practice set we will generate some dummy data then classify using a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to calculate build ROC curve\n",
    "def plot_ROC(y_pred_proba, y_actual, steps=100, debug=False):\n",
    "    # convert y_actual\n",
    "    y_actual = np.array(y_actual)\n",
    "    # instantiate fpr and tpr arrays\n",
    "    fpr = []\n",
    "    tpr = []\n",
    "    # step through descrimation threshold from 0-1 by step_size\n",
    "    for thresh in np.linspace(0, 1, num=steps, endpoint=False):\n",
    "        # assign True if probability >= threshold, else False\n",
    "        y_pred = np.array([proba>=thresh for proba in y_pred_proba])\n",
    "        # calculate tp, fp, fn, and tn counts\n",
    "        tp = ((y_pred==True) & (y_actual==True)).sum()\n",
    "        fp = ((y_pred==True) & (y_actual==False)).sum()\n",
    "        fn = ((y_pred==False) & (y_actual==True)).sum()\n",
    "        tn = ((y_pred==False) & (y_actual==False)).sum()\n",
    "        # calculate and append tpr and fpr\n",
    "        if (tp+fn) > 0 and (fp+tn) > 0:\n",
    "            tpr.append(float(tp)/(tp+fn))\n",
    "            fpr.append(float(fp)/(fp+tn))\n",
    "        if debug==True:\n",
    "            print \"{:.2}\\t{:.2}\\t{:.2}\".format(thresh, tpr[-1], fpr[-1])\n",
    "    # plot ROC curve\n",
    "    plt.plot(fpr, tpr, 'b-')\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y1_pred_proba = [p[1] for p in y_pred_proba]\n",
    "y1_test = [True if y==1 else False for y in y_test]\n",
    "\n",
    "plot_ROC(y1_pred_proba, y1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Plot Precision as a function of TPR (aka PR curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function to calculate build prec vs tpr curve (aka PR curve)\n",
    "def plot_PR(y_pred_proba, y_actual, steps=100, debug=False):\n",
    "    # convert y_actual\n",
    "    y_actual = np.array(y_actual)\n",
    "    # instantiate precision and recall arrays\n",
    "    prec = []\n",
    "    rec = []\n",
    "    # step through descrimation threshold from 0-1 by step_size\n",
    "    for thresh in np.linspace(0, 1, num=steps, endpoint=False):\n",
    "        # assign True if probability >= threshold, else False\n",
    "        y_pred = np.array([proba>=thresh for proba in y_pred_proba])\n",
    "        # calculate tp, fp, fn, and tn counts\n",
    "        tp = ((y_pred==True) & (y_actual==True)).sum()\n",
    "        fp = ((y_pred==True) & (y_actual==False)).sum()\n",
    "        fn = ((y_pred==False) & (y_actual==True)).sum()\n",
    "        tn = ((y_pred==False) & (y_actual==False)).sum()\n",
    "        # calculate and append tpr and fpr\n",
    "        if (tp+fp) > 0 and (tp+fn) > 0:\n",
    "            prec.append(float(tp)/(tp+fp))\n",
    "            rec.append(float(tp)/(tp+fn))\n",
    "        if debug==True:\n",
    "            print \"{:.2}\\t{:.2}\\t{:.2}\".format(thresh, prec[-1], rec[-1])\n",
    "    # plot ROC curve\n",
    "    plt.plot(rec, prec, 'b-')\n",
    "    plt.title(\"PR Curve\")\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y1_pred_proba = [p[1] for p in y_pred_proba]\n",
    "y1_test = [True if y==1 else False for y in y_test]\n",
    "\n",
    "plot_PR(y1_pred_proba, y1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Given priors, find operating point of a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we knew the prior probabilities of a true event, how would this influence the operating point (aka discrimination threshold) of our classifier?\n",
    "\n",
    "Let's run through a common example: we want our classifier to catch a some disease but this disease occurs in 1% of the population we're looking at.\n",
    "\n",
    "We can generate an example dataset given this information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior = (actual true)/(dataset) = (TP+FN)/(TP+FP+FN+TN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate an unbalanced dataset with 1% positive\n",
    "pos_prob = 0.01\n",
    "X, y = make_unbalanced_blobs(pos_prob)\n",
    "\n",
    "# view a sample dataset\n",
    "x1 = [i[0] for i in X]\n",
    "x2 = [i[1] for i in X]\n",
    "\n",
    "plt.scatter(x1, x2, c=y)\n",
    "plt.title(\"x2 vs x1 in unbalanced dataset\")\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's offload some tasks to a sklearn Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Confirm that PR depends on tested priors, but ROC does not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run through an increasing array of probabilities\n",
    "pos_probs = [0.01, 0.05, 0.1, 0.25]\n",
    "\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "prec = {}\n",
    "rec = {}\n",
    "\n",
    "for prob in pos_probs:\n",
    "    X, y = make_unbalanced_blobs(prob)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n",
    "\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y1_pred_proba = [p[1] for p in clf.predict_proba(X_test)]\n",
    "\n",
    "    fpr[str(prob)], tpr[str(prob)], thresholds = roc_curve(y_test, y1_pred_proba)\n",
    "    prec[str(prob)], rec[str(prob)], thresholds = precision_recall_curve(y_test, y1_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for p in [str(i) for i in pos_probs]:\n",
    "    plt.plot(fpr[p], tpr[p], label=p)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for p in [str(i) for i in pos_probs]:\n",
    "    plt.plot(rec[p], prec[p], label=p)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
